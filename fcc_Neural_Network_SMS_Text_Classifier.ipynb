{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8RZOuS9LWQvv"},"outputs":[],"source":["# import libraries\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  !pip install tf-nightly\n","except Exception:\n","  pass\n","import tensorflow as tf\n","import pandas as pd\n","from tensorflow import keras\n","!pip install tensorflow-datasets\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMHwYXHXCar3"},"outputs":[],"source":["# get data files\n","!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n","!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n","\n","train_file_path = \"train-data.tsv\"\n","test_file_path  = \"valid-data.tsv\""]},{"cell_type":"code","source":["names = [\"class\", \"message\"]"],"metadata":{"id":"Yf8JPZzTJl8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_h508FEClxO"},"outputs":[],"source":["df_train = pd.read_csv(train_file_path, sep=\"\\t\", names=names)\n","df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOMKywn4zReN"},"outputs":[],"source":["df_test = pd.read_csv(test_file_path, sep=\"\\t\", names=names)\n","df_test.head()"]},{"cell_type":"code","source":["print(len(df_train))\n","print(len(df_test))"],"metadata":{"id":"cYWRxJoot4Cc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Handle Data and Exploring Data"],"metadata":{"id":"cjTl4plGvDdr"}},{"cell_type":"code","source":["y_train = np.array([0 if x==\"ham\" else 1 for x in df_train['class']])\n","y_test  = np.array([0 if x==\"ham\" else 1 for x in df_test['class']])"],"metadata":{"id":"As_3z9WrONTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bar = df_train['class'].value_counts()\n","\n","plt.bar(bar.index, bar)\n","plt.xlabel('Class')\n","plt.title('Number of ham and spam messages')"],"metadata":{"id":"kGcqjcW_ukId"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Text processing"],"metadata":{"id":"SKgBhicIyjcw"}},{"cell_type":"code","source":["import nltk\n","import re\n","import string\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from string import punctuation\n","from nltk.stem import PorterStemmer, WordNetLemmatizer"],"metadata":{"id":"Ue_mCdrlyq6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords') # download stopwords\n","nltk.download('wordnet')   # download vocab for lemmatizer"],"metadata":{"id":"bNmpUrtizbB9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","def preprocess_text(text):\n","    # Converts text to lowercase\n","    text = text.lower()\n","\n","    # Remove punctuation\n","    text = \"\".join([char for char in text if char not in punctuation])\n","\n","    # Removes stop words (common, uninformative words like \"the\", \"a\")\n","    stop_words = stopwords.words('english')\n","    text = \" \".join([word for word in text.split() if word not in stop_words])\n","\n","    # Stemming\n","    stemmer = PorterStemmer()\n","    text = \" \".join([stemmer.stem(word) for word in text.split()])\n","\n","    # Lemmatizes text using WordNetLemmatizer\n","    text = re.sub(r'([^\\s\\w])+', ' ', text)\n","    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()\n","                    if not word in stop_words])\n","\n","    # words = text.split()\n","    # pos_tags = pos_tag(words)\n","    # lemmatized_words = [lemmatizer.lemmatize(word, pos='n' if pos.startswith('N') else pos) for word, pos in pos_tags]\n","    # text = \" \".join(lemmatized_words)\n","\n","    return text"],"metadata":{"id":"6s20J8kM09jJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = df_train['message'].apply(lambda x: preprocess_text(x))\n","X_train[:5]\n","\n","X_test = df_test['message'].apply(lambda x: preprocess_text(x))\n","X_test[:5]"],"metadata":{"id":"mvKIrsGl9wq5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Tokenization"],"metadata":{"id":"0rzN43ZkFJ9f"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"Iqd8lgBzDXkJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define tokenization parameters\n","# Consider adjusting based on data size and complexity\n","num_words = 1000\n","\n","# Cut off the words after seeing 500 words in each document\n","max_len = 500\n","\n","# Create and fit tokenizer\n","tokenizer = Tokenizer(num_words=num_words)\n","tokenizer.fit_on_texts(X_train)"],"metadata":{"id":"27KM_k1kGX-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform each text to a sequence of integers\n","sequences = tokenizer.texts_to_sequences(X_train)\n","sequences[:5]"],"metadata":{"id":"YUiwQjW6GqjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make all rows of equal length\n","X_train_vec = sequence.pad_sequences(sequences, maxlen=max_len)\n","X_train_vec[:5]"],"metadata":{"id":"7ejhVX7THZDV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Create Model"],"metadata":{"id":"HNnXN2YUHud5"}},{"cell_type":"code","source":["i = tf.keras.layers.Input(shape=[max_len])\n","x = tf.keras.layers.Embedding(num_words, 50)(i)\n","x = tf.keras.layers.LSTM(64)(x) # Adjust hidden units\n","\n","x = tf.keras.layers.Dense(256, activation='relu')(x)\n","x = tf.keras.layers.Dropout(0.5)(x)\n","x = tf.keras.layers.Dense(1, activation='sigmoid')(x) # Output layer for binary classification\n","\n","model = tf.keras.models.Model(inputs=i, outputs=x)\n","model.compile(loss='binary_crossentropy',optimizer='RMSprop',metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"oisXJbBWJT_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_fit = model.fit(X_train_vec, y_train,\n","              batch_size=128, epochs=10,\n","              validation_split=0.2,\n","              callbacks=[tf.keras.callbacks.EarlyStopping(\n","              monitor='val_loss', min_delta=0.0001)])"],"metadata":{"id":"AOMNLnbZNnM6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Evaluate and Improve (Optional):"],"metadata":{"id":"-pgf24PiOVUm"}},{"cell_type":"code","source":["plt.plot(model_fit.history['loss'], label='loss')\n","plt.plot(model_fit.history['val_loss'], label='val_loss')\n","plt.legend()"],"metadata":{"id":"KAsjiKFJOTed"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"rpea4CK7OeDp"}},{"cell_type":"code","source":["plt.plot(model_fit.history['accuracy'], label='acc')\n","plt.plot(model_fit.history['val_accuracy'], label='val_acc')\n","plt.legend()"],"metadata":{"id":"k0ljQyCDOexL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocessing(X):\n","  x = X.apply(lambda x: preprocess_text(x))\n","  x = tokenizer.texts_to_sequences(x)\n","  return sequence.pad_sequences(x, maxlen=max_len)"],"metadata":{"id":"rvBTX7ULQ8PG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s = model.evaluate(preprocessing(df_test['message']), y_test)"],"metadata":{"id":"OAwOdGvqRHh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9tD9yACG6M9"},"outputs":[],"source":["# function to predict messages based on model\n","# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n","def predict_message(pred_text):\n","  p = model.predict(preprocessing(pd.Series([pred_text])))[0]\n","\n","  return (p[0], (\"ham\" if p<0.5 else \"spam\"))\n","\n","pred_text = \"how are you doing today?\"\n","\n","prediction = predict_message(pred_text)\n","print(prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dxotov85SjsC"},"outputs":[],"source":["# Run this cell to test your function and model. Do not modify contents.\n","def test_predictions():\n","  test_messages = [\"how are you doing today\",\n","                   \"sale today! to stop texts call 98912460324\",\n","                   \"i dont want to go. can we try it a different day? available sat\",\n","                   \"our new mobile video service is live. just install on your phone to start watching.\",\n","                   \"you have won Â£1000 cash! call to claim your prize.\",\n","                   \"i'll bring it tomorrow. don't forget the milk.\",\n","                   \"wow, is your arm alright. that happened to me one time too\"\n","                  ]\n","\n","  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n","  passed = True\n","\n","  for msg, ans in zip(test_messages, test_answers):\n","    prediction = predict_message(msg)\n","    if prediction[1] != ans:\n","      passed = False\n","\n","  if passed:\n","    print(\"You passed the challenge. Great job!\")\n","  else:\n","    print(\"You haven't passed yet. Keep trying.\")\n","\n","test_predictions()\n"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/freeCodeCamp/boilerplate-neural-network-sms-text-classifier/blob/master/fcc_sms_text_classification.ipynb","timestamp":1707300794767}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{}},"nbformat":4,"nbformat_minor":0}